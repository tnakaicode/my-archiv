\documentclass[paper.tex]{subfiles}

\begin{document}

\section{Error and cost models}
\label{sec:error-and-cost-models}

The input to \name is a floating-point expression $\tilde{E}$,
  with no loops or control flow,
  over some number of floating-point inputs;
  for simplicity, this section refers to a single input $x_0$,
  though generalizing to multiple inputs is straightforward.
In addition to the expression $\tilde{E}$,
  the input must also contain an interval $I$
  that the input $x_0$ is drawn from.
The core idea behind \name,
  as described in \Cref{sec:big-idea},
  is the linear error model for a floating-point expression.
To compute the error model \name uses error Taylor series~\cite{fptaylor}.

\subsection{Error model}

Take a floating-point expression $\tilde{E}$
  generated by the grammar $e ::= \tilde{f}(e, e, \ldots) \mid x_0$.
In this paper, variables with tildes, like $\tilde{x}_i$,
  represent floating-point values,
  and the same variables without tildes, like $x_i$,
  represent their true mathematical value
  when computed without rounding error.
Each $\tilde{f}$ thus implements of some underlying function $f$;
  floating-point constants like \F{PI}
  can be considered zero-argument functions.

Transform the expression $\tilde{E}$ to a linear sequence of function calls
  $\tilde{x}_n := \tilde{f_n}(\tilde{x}_{n-a_n}, \tilde{x}_{n-b_n}, \ldots)$,
  where later function calls
  may use the results of earlier calls as arguments,
  and where the value of the expression as a whole
  is the value of the last variable in the sequence, $\tilde{x}_N$.
By \cref{rdmodel},
  $\tilde{x}_n = f_n(\tilde{x}_{n-a_n}, \tilde{x}_{n-b_n}, \ldots)(1 + \varepsilon_n) + \delta_n$.
Note that $\tilde{x}_N$ is thus a function of the $\varepsilon_n$s and $\delta_n$s.
When all $\varepsilon_n = \delta_n = 0$,
  the result is $x_n$ computed without any error;
  but $\varepsilon_n$ and $\delta_n$ are not equal to zero.
Instead they are small, unkown values
  bounded by constants $\varepsilon_{f_n}$ and $\delta_{f_n}$.

Define the constants
\[
A^k_n = \max_{x \in I} \left( \frac{\partial \tilde{x}_k}{\partial \varepsilon_n}
\left[\begin{array}{l}0 / \varepsilon \\ 0 / \delta \end{array}\right] \right)
\;\text{and}\;
B^k_n = \max_{x \in I} \left( \frac{\partial \tilde{x}_k}{\partial \delta_n}
\left[\begin{array}{l}0 / \varepsilon \\ 0 / \delta \end{array}\right] \right)
\]
  and $A_n = A^N_n$ and $B_n = B^N_n$.
Note that these constants are real numbers
  and can be computed
  by a one-dimensional global nonlinear optimization;
  or, for expressions with multiple variables,
  over as many dimensions as there are input variables.
Then the error of $\tilde{E}$ is:
\begin{equation}\label{errmodelformal}
  |\tilde{x}_N - x_N| \le \sum_n A_n \varepsilon_{f_n} + \sum_n B_n
  \delta_{f_n} + o(\varepsilon^2).
\end{equation}
Here, the $o(\varepsilon^2)$ term represents
  the quadratic and higher-order terms
  of the error Taylor series.
It can be bounded using Lagrange's theorem,
\[
  o(\varepsilon^2) \le
  \max_{x \in I} \left(
  \sum_{u, v \in \{\varepsilon, \delta\}}
  \sum_{i,j}
  \frac{\partial^2 x_k}{\partial u_i \partial v_i}
    \left[\begin{array}{l}0 / \varepsilon \\ 0 / \delta \end{array}\right]
    u_{f_i}
    v_{f_j}
    \right),
\]
  where the sums range over all pairs of $\varepsilon/\delta$ variables.
Importantly, since this bound involves higher powers
  of the $\varepsilon_f$s and $\delta_f$s,
  it tends to be insignificantly small.
It is necessary for soundness,
  but is not particularly important for estimating the error.

\Cref{errmodelformal},
  including the Lagrange bound for the higher-order terms,
  is the traditional use of error Taylor series.
\name instead drops the higher-order terms
  and considers the remaining linear terms as
  a function of the $\varepsilon$ and $\delta$ variables
  that estimates $\tilde{E}$'s error
  in terms of the function implementations it uses.
Note that typically, an expression contains operators
  like addition and multiplication that \name cannot tune.
In this case
  some of the $\varepsilon$ and $\delta$s are fixed
  and \name folds those resulting terms into a constant.

\subsection{Cost model}

The complement to the linear error model is a linear cost model,
  which estimates the speed of the expression
  given the implementations chosen for each function call.
\name uses a simple cost model:
\[
\mathit{cost} = \sum_n c_{f_n},
\]
  where $c_f$ is the average runtime of the function implementation $\tilde{f}$.
To measure $c_f$, each implementation
  is compiled and run
  on \nTimingInputs random valid inputs
  in a tight loop for \nTimingElapsed.
To try to get maximally accurate timings.
  we use multiple measurements, input pre-generation, core binding,
  and the \texttt{-O3 -march=native -mtune=native -DNDEBUG} compiler flags.

Normally, a simple sum of average runtimes is too simplistic:
  it ignores the complexities of modern CPUs
  and the details of input-dependent control flow.
Several factors make this model more appropriate in our setting.
\name's goal is to find
  all Pareto-optimal trade-offs between the error and cost models,
  so the cost model only needs to be a \textit{relative order},
  not a precise prediction of runtimes.
Plus, mathematical function implementations usually
  have rare, easily-branch-predicted control flow;
  use few if any data structures or complex memory access patterns;
  and are generally compute-bound.
That makes average runtimes more meaningful
  than for general-purpose code.
Finally, since \name only changes the implementation of mathematical functions,
  any computation outside a function implementation
  will be the same across all configurations.
The cost model thus does not need to model those costs,
  and is only predicting the costs of calls to shared libraries.

One caveat is necessary for achieving good results with this cost model:
  the linear sequence of function calls $x_n = \tilde{f}_n(x_{n-a_n}, x_{n-b_n}, \ldots)$
  must contain no duplicate entries representing common subexpressions.
Compilers commonly perform common subexpression elimination
  to avoid recomputing the same expression twice.
Thus, if two subexpressions both compute $f(x_1, x_2)$,
  and the first subexpression uses implementation $\tilde{f}_a$
  while the second uses implementation $\tilde{f}_b$,
  the cost is $c_a + c_b$ if these implementations are different
  but only $c_a$ is both implementations are the more accurate of the two.
In other words, when common subexpressions exist,
  it's best to use the same, more accurate implementation at both sites.
\name thus performs common subexpression elimination
  when converting expressions to a linear sequence of function calls,
  and the cost and error models are built on this deduplicated sequence,
  ensuring that common subexpressions use the same implementations.

\subsection{Mathematical Libraries}
\label{ssec:libraries}

\newcommand{\mr}[2]{\multirow{#1}{*}{#2}}
\newcommand{\mrd}[2]{\multirow{#1}{2em}{#2}}

\begin{figure}
\begin{subfigure}[t]{0.5\linewidth}
{\footnotesize
\begin{tabular}[t]{|c|l|r|r|l|} \hline
Func         & Domain                                   & Error       & Cost  & Library  \\ \hline
\mr{5}{exp}  & \mrd{5}{$[-1.79e^{308},$ $709.78]$}      & 0.5         & 54.02 & CRLibM   \\ \cline{3-5}
             &                                          & \mr{3}{1.0} & 10.71 & GLibC    \\ \cline{4-5}
             &                                          &             & 10.70 & OpenLibm \\ \cline{4-5}
             &                                          &             & 10.78 & AMD LibM \\ \cline{3-5}
             &                                          & 5.0         & 5.32  & VDT      \\ \hline
\mr{5}{log}  & \mrd{5}{$[5.0e^{-324},$ $1.79e^{30}]$}   & 0.5         & 32.93 & CRLibM   \\ \cline{3-5}
             &                                          & \mr{3}{1.0} & 8.53  & GLibC    \\ \cline{4-5}
             &                                          &             & 8.53  & OpenLibm \\ \cline{4-5}
             &                                          &             & 8.36  & AMD LibM \\ \cline{3-5}
             &                                          & 5.0         & 5.99  & VDT      \\ \hline
\mr{6}{sin}  & \mrd{5}{$[-1.79e^{308},$ $1.79e^{308}]$} & 0.5         & 35.27 & CRLibM   \\ \cline{3-5}
             &                                          & \mr{3}{1.0} & 8.76  & GLibC    \\ \cline{4-5}
             &                                          &             & 8.76  & OpenLibm \\ \cline{4-5}
             &                                          &             & 7.56  & AMD LibM \\ \cline{3-5}
             &                                          & 5.0         & 4.42  & VDT      \\ \cline{2-5}
             & \mr{1}{\tiny$[-0.78, 0.78]$}             & 5.0         & 1.82  & VDT      \\ \hline
\mr{6}{cos}  & \mrd{5}{$[-1.79e^{308},$ $1.79e^{308}]$} & 0.5         & 34.35 & CRLibM   \\ \cline{3-5}
             &                                          & \mr{3}{1.0} & 8.84  & GLibC    \\ \cline{4-5}
             &                                          &             & 8.82  & OpenLibm \\ \cline{4-5}
             &                                          &             & 7.19  & AMD LibM \\ \cline{3-5}
             &                                          & 5.0         & 4.20  & VDT      \\ \cline{2-5}
             & \mr{1}{\tiny$[-0.78, 0.78]$}             & 5.0         & 2.04  & VDT      \\ \hline
\mr{5}{tan}  & \mrd{5}{$[-1.79e^{308},$ $1.79e^{308}]$} & 0.5         & 80.20 & CRLibM   \\ \cline{3-5}
             &                                          & \mr{3}{1.0} & 15.81 & GLibC    \\ \cline{4-5}
             &                                          &             & 16.00 & OpenLibm \\ \cline{4-5}
             &                                          &             & 28.66 & AMD LibM \\ \cline{3-5}
             &                                          & 5.0         & 8.90  & VDT      \\ \hline
\end{tabular}
}
\subcaption{Double precision implementations}
\end{subfigure}\hfill%
\begin{subfigure}[t]{0.5\linewidth}
{\footnotesize
\begin{tabular}[t]{|c|l|r|r|l|} \hline
Func         & Domain                                 & Error                       & Cost & Library  \\ \hline
\mr{5}{expf} & \mrd{5}{$[-3.40e^{38},$ $88.72]$}      & $\float{2.68}{8}$           & 5.06 & RLibM    \\ \cline{3-5}
             &                                        & \mr{3}{$\float{4.03}{8}$}   & 8.46 & GLibC    \\ \cline{4-5}
             &                                        &                             & 4.86 & OpenLibm \\ \cline{4-5}
             &                                        &                             & 4.68 & AMD LibM \\ \cline{3-5}
             &                                        & $\float{1.76}{10}$          & 5.20 & VDT      \\ \hline
\mr{5}{logf} & \mrd{5}{$[1.40e^{-45},$ $3.40e^{38}]$} & $\float{2.68}{8}$           & 6.26 & RLibM    \\ \cline{3-5}
             &                                        & \mr{3}{$\float{2.76}{8}$}   & 7.15 & GLibC    \\ \cline{4-5}
             &                                        &                             & 7.13 & OpenLibm \\ \cline{4-5}
             &                                        &                             & 5.88 & AMD LibM \\ \cline{3-5}
             &                                        & $\float{5.45}{8}$           & 5.80 & VDT      \\ \hline
\mr{6}{sinf} & \mrd{5}{$[-3.40e^{38},$ $3.40e^{38}]$} & \mr{3}{$\float{2.16}{9}$}   & 7.13 & GLibC    \\ \cline{4-5}
             &                                        &                             & 7.10 & OpenLibm \\ \cline{4-5}
             &                                        &                             & 7.64 & AMD LibM \\ \cline{3-5}
             &                                        & $\float{1.74}{10}$          & 4.69 & VDT      \\ \cline{2-5}
             & \mr{1}{\tiny $[-0.785, 0.785]$}        & $\float{1.74}{10}$          & 1.60 & VDT      \\ \hline
\mr{6}{cosf} & \mrd{5}{$[-3.40e^{38},$ $3.40e^{38}]$} & \mr{3}{$\float{2.16}{9}$}   & 7.13 & GLibC    \\ \cline{4-5}
             &                                        &                             & 7.10 & OpenLibm \\ \cline{4-5}
             &                                        &                             & 6.76 & AMD LibM \\ \cline{3-5}
             &                                        & $\float{1.74}{10}$          & 4.47 & VDT      \\ \cline{2-5}
             & \mr{1}{\tiny $[-0.785, 0.785]$}        & $\float{1.74}{10}$          & 1.93 & VDT      \\ \hline
\mr{5}{tanf} & \mrd{5}{$[-3.40e^{38},$ $3.40e^{38}]$} & \mr{3}{$\float{5.37}{8}$}   & 8.50 & GLibC    \\ \cline{4-5}
             &                                        &                             & 8.49 & OpenLibm \\ \cline{4-5}
             &                                        &                             & 7.03 & AMD Libm \\ \cline{3-5}
             &                                        & $\float{1.77}{10}$          & 8.32 & VDT      \\ \hline
\end{tabular}
}
\subcaption{Single precision implementations}
\end{subfigure}

\caption{
  The standard library implementations available to \name.
  Errors were found via documentation and source code comments
    and are given in ``units in the last place'';
    the actual $\varepsilon_f$ and $\delta_f$ values
    are the listed error times $2^{-52}$ for $\varepsilon_f$ and $2^{-1022}$ for $\delta_f$.
  There is a general trade-off between speed and accuracy,
    though with a lot of heterogeneity between libraries and functions.
}
\label{fig:table}
\end{figure}


Implementations of mathematical functions
  are usually gathered into libraries
  that provide a large collection of functions
  all at a similar point in the speed-accuracy trade-off.
These libraries broadly cover a spectrum
  from faster, less-accurate implementations
  to slower, more-accurate ones.
\name ships with support for many of the most popular libraries,
  covering a range of accuracies and speeds;
  \Cref{fig:table} lists these libraries
  and their error and cost model parameters
  $D$, $\varepsilon_f$, $\delta_f$, and $c_f$.

% CRLibM, RLibM
% * usually slow, RLibM breaks this
% * RLibM is magic, we need more magic (doesn't help our message, but I love Jay's work)
The golden standard for function implementation are correctly rounded
   implementations.
These yield the true mathematical result, rounded to floating-point;
  this value is unique,
  so all correctly-rounded libraries produce identical answers on all inputs.
Unfortunately, achieving this accuracy is still a topic of active research.
CRLibM provides correctly-rounded implementations
  of many math functions in double precision,
  but it is quite slower, usually by a factor of $5$--$10\times$,
  than a traditional implementation.
Work on reducing this overhead is ongoing.
The recently-published RLibM library
  achieves correct rounding and speed comparable to alternative libraries
  using techniques from program synthesis and verification.
However, its techniques do not scale to 64-bit implementations,
  so RLibM only provides 32-bit implementations
  and furthermore only for a small set of mathematical functions
  not including, for example, \F{sin} and \F{cos}.
Ultimately, neither CRLibM or Rlibm is (currently) in common use, showing that
  most users have already decided on higher speed in exchange for lower
  accuracy.

% GLibC, OpenLibm, AMDLibm
% * includes both the old boys like GLibC and young ones like AMD
% * usually faster than correct by a factor ~4
Standard system math libraries aim for nearly-correct rounding.
These libraries, which include GLibC, OpenLibm, and AMD Libm,
  aim to achieve the highest possible performance
  while allowing the least- or even second-least-significant bit to be incorrect.
Being marginally less accurate than CRLibM and RLibM
  often allows them to be dramatically faster.
Consequently, these libraries are appropriate for general-purpose code,
  where speed is a top-level concern
  but where programmers do not have the expertise
  to optimize the speed-accuracy trade-off further.
Different libraries in this category
  use different implementation strategies
  in order to achieve maximum speed,
  including (often) custom implementations
  for specific hardware architectures or generations.
For example, GLibC has roughly a dozen
  architecture-specific implementations of the main mathematical functions.

% VDT
% * takes advantage of excess precision
% * can only be used by expert
% * usually 2x faster than standard
In some domains, greater speed and lower accuracy are required;
  for such programs, special-purpose mathematical libraries exist.
For example, the VDT library, developed by CERN,
  allows for up to 3 incorrect bits,
  two more than the standard system library
  (more in single precision).
In exchange for lower accuracy,
  VDT can be up to twice as fast
  as the corresponding GLibC function
  and also has some vectorization advantages.
VDT uses Pad\'e approximations (a variant on polynomial approximations)
  and tunes them to slightly lower accuracy in order to achieve this speedup.

Finally, some applications are best run
  at a speed-accuracy trade-off not represented
  by any of the above libraries.
For such cases, MetaLibm provides a metaprogramming framework
  ideal for generating new function implementations.
MetaLibm provides easy access to polynomial generation with Sollya,
  a convenient cross-compilation mechanism,
  and utility routines useful for writing function implementations.
Of particular interest is that MetaLibm implementations
  can be parameterized;
  for example, MetaLibm's \F{exp} implementation
  can be parameterized by any number of polynomial terms.
Besides the specific implementations that ship with MetaLibm,
  such as the parameterized \F{exp} implementation,
  users can also use MetaLibm to write their own custom implementations.

% Float versions
% Fast versions
% Restricted range versions
Some libraries also provide multiple implementations of the same function
  for tuning purposes.
The simplest case of this is providing different implementations
  for single- and double-precision arguments:
  single-precision versions are fundamentally less accurate due to rounding,
  so different parameter choices are advised.
The single-precision versions also usually have a narrow input range.
Usually the single-precision version is not implemented independently;
  instead it uses a truncated and rounded form
  of the polynomial used for the double function.
(This is not optimal, but economizes on the high cost
  of developing a novel function implementation.)
Some libraries also provide function implementations
  valid over only a certain input range,
  which allows those implementations to skip (or simplify) range reduction.
For example, the VDT library internally
  contains implementations of \F{sin} and \F{cos} which are valid
  only on the reduced range $[-\pi/2, \pi/2]$.
These types of implementations are intended to be used with external range
  reduction, and are often not exposed at a library level, but using them
  directly can lead to generous advantages in speed.
The bar to using these implementations
  is proving that the input will be in the reduced range.

%% \todo{Eh}
%% In sum,
%%   the diverse collection of math libraries
%%   available to the numerical software developer
%%   lets developers choose the right point
%%   on the speed-accuracy trade-off
%%   for their implementation---%
%%   assuming they have the expertise
%%   to evaluate the options and pick the best one.


\end{document}
