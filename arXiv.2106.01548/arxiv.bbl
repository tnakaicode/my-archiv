\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akbari et~al.(2021)Akbari, Yuan, Qian, Chuang, Chang, Cui, and
  Gong]{akbari2021vatt}
Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin
  Cui, and Boqing Gong.
\newblock Vatt: Transformers for multimodal self-supervised learning from raw
  video, audio and text.
\newblock \emph{arXiv preprint arXiv:2104.11178}, 2021.

\bibitem[Arnab et~al.(2021)Arnab, Dehghani, Heigold, Sun, Lu{\v{c}}i{\'c}, and
  Schmid]{arnab2021vivit}
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu{\v{c}}i{\'c},
  and Cordelia Schmid.
\newblock Vivit: A video vision transformer.
\newblock \emph{arXiv preprint arXiv:2103.15691}, 2021.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{layer-norm}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bello et~al.(2021)Bello, Fedus, Du, Cubuk, Srinivas, Lin, Shlens, and
  Zoph]{bello2021revisiting}
Irwan Bello, William Fedus, Xianzhi Du, Ekin~D. Cubuk, Aravind Srinivas,
  Tsung-Yi Lin, Jonathon Shlens, and Barret Zoph.
\newblock Revisiting resnets: Improved training and scaling strategies, 2021.

\bibitem[Bertasius et~al.(2021)Bertasius, Wang, and
  Torresani]{bertasius2021space}
Gedas Bertasius, Heng Wang, and Lorenzo Torresani.
\newblock Is space-time attention all you need for video understanding?
\newblock \emph{arXiv preprint arXiv:2102.05095}, 2021.

\bibitem[Beyer et~al.(2020)Beyer, H{\'e}naff, Kolesnikov, Zhai, and
  Oord]{imagenet-real}
Lucas Beyer, Olivier~J H{\'e}naff, Alexander Kolesnikov, Xiaohua Zhai, and
  A{\"a}ron van~den Oord.
\newblock Are we done with imagenet?
\newblock \emph{arXiv preprint arXiv:2006.07159}, 2020.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{aleksandar2017practical}
Aleksandar Botev, Hippolyt Ritter, and David Barber.
\newblock Practical {G}auss-{N}ewton optimisation for deep learning.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 557--565. PMLR, 06--11
  Aug 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/botev17a.html}.

\bibitem[Burkholz and Dubatovka(2019)]{burkholz2019init}
Rebekka Burkholz and Alina Dubatovka.
\newblock Initialization of relus for dynamical isometry.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf}.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, Jégou, Mairal, Bojanowski,
  and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr
  Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers, 2021.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2017entropysgd}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 1597--1607. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/chen20j.html}.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Gong, and Wang]{chen2021neural}
Wuyang Chen, Xinyu Gong, and Zhangyang Wang.
\newblock Neural architecture search on imagenet in four {GPU} hours: A
  theoretically inspired perspective.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=Cnon5ezMHtu}.

\bibitem[Chen and Hsieh(2020)]{pmlr-v119-chen20f}
Xiangning Chen and Cho-Jui Hsieh.
\newblock Stabilizing differentiable architecture search via perturbation-based
  regularization.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 1554--1565. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/chen20f.html}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Xie, Tan, Zhang, Hsieh, and
  Gong]{chen2021robust}
Xiangning Chen, Cihang Xie, Mingxing Tan, Li~Zhang, Cho-Jui Hsieh, and Boqing
  Gong.
\newblock Robust and accurate object detection via adversarial learning,
  2021{\natexlab{b}}.

\bibitem[Chen et~al.(2021{\natexlab{c}})Chen, Xie, and He]{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers,
  2021{\natexlab{c}}.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2019autoaugment}
Ekin~D. Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V. Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2019.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2019randaugment}
Ekin~D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V. Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)}, pages 3008--3017, 2020.
\newblock \doi{10.1109/CVPRW50498.2020.00359}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255, 2009.
\newblock \doi{10.1109/CVPR.2009.5206848}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In Gal Elidan, Kristian Kersting, and Alexander~T. Ihler, editors,
  \emph{Proceedings of the Thirty-Third Conference on Uncertainty in Artificial
  Intelligence, {UAI} 2017, Sydney, Australia, August 11-15, 2017}. {AUAI}
  Press, 2017.
\newblock URL \url{http://auai.org/uai2017/proceedings/papers/173.pdf}.

\bibitem[Fan et~al.(2021)Fan, Xiong, Mangalam, Li, Yan, Malik, and
  Feichtenhofer]{fan2021multiscale}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock \emph{arXiv preprint arXiv:2104.11227}, 2021.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{He2020moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJz6tiCqYm}.

\bibitem[Hendrycks and Gimpel(2020)]{hendrycks2020gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus), 2020.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, Song, Steinhardt, and Gilmer]{hendrycks2020faces}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob
  Steinhardt, and Justin Gilmer.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization, 2020.

\bibitem[Jastrzębski et~al.(2019)Jastrzębski, Kenton, Ballas, Fischer,
  Bengio, and Storkey]{jastrzebski2018on}
Stanisław Jastrzębski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua
  Bengio, and Amost Storkey.
\newblock On the relation between the sharpest directions of {DNN} loss and the
  {SGD} step length.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=SkgEaj05t7}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017largebatch}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{khosla2020supcon}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 18661--18673. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf}.

\bibitem[Kingma and Ba(2015)]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and Yuan]{pmlr-v80-kleinberg18a}
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does {SGD} escape local minima?
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 2698--2707. PMLR,
  10--15 Jul 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/kleinberg18a.html}.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{alexander202bit}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
  editors, \emph{Computer Vision -- ECCV 2020}, pages 491--507, Cham, 2020.
  Springer International Publishing.
\newblock ISBN 978-3-030-58558-7.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{li2018visualize}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Dai, So, and Le]{liu2021pay}
Hanxiao Liu, Zihang Dai, David~R So, and Quoc~V Le.
\newblock Pay attention to mlps.
\newblock \emph{arXiv preprint arXiv:2105.08050}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows, 2021{\natexlab{b}}.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJzIBfZAb}.

\bibitem[Melas-Kyriazi(2021)]{melas2021you}
Luke Melas-Kyriazi.
\newblock Do you even need attention? a stack of feed-forward layers does
  surprisingly well on imagenet.
\newblock \emph{arXiv preprint arXiv:2105.02723}, 2021.

\bibitem[Nesterov(1983)]{Nesterov1983AMF}
Y.~Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate $o(1/k^2)$.
\newblock \emph{Proceedings of the USSR Academy of Sciences}, 269:\penalty0
  543--547, 1983.

\bibitem[Nilsback and Zisserman(2008)]{nilsback2008flower}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian Conference on Computer Vision, Graphics
  Image Processing}, pages 722--729, 2008.
\newblock \doi{10.1109/ICVGIP.2008.47}.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and C.~V. Jawahar.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 3498--3505, 2012.
\newblock \doi{10.1109/CVPR.2012.6248092}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and Sutskever]{gpt}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{imagenet-v2}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, pages
  5389--5400. PMLR, 2019.

\bibitem[Shafahi et~al.(2019)Shafahi, Najibi, Ghiasi, Xu, Dickerson, Studer,
  Davis, Taylor, and Goldstein]{shafahi2019free}
Ali Shafahi, Mahyar Najibi, Mohammad~Amin Ghiasi, Zheng Xu, John Dickerson,
  Christoph Studer, Larry~S Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/7503cfacd12053d309b6bed5c89de212-Paper.pdf}.

\bibitem[Shin and Karniadakis(2020)]{shin2020trainability}
Yeonjong Shin and George~Em Karniadakis.
\newblock Trainability of relu networks and data-dependent initialization.
\newblock \emph{Journal of Machine Learning for Modeling and Computing},
  1\penalty0 (1):\penalty0 39--74, 2020.
\newblock ISSN 2689-3967.

\bibitem[Smith and Le(2018)]{l.2018a}
Samuel~L. Smith and Quoc~V. Le.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJij4yg0Z}.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava14dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun2017revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{2017 IEEE International Conference on Computer Vision
  (ICCV)}, pages 843--852, 2017.
\newblock \doi{10.1109/ICCV.2017.97}.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016inception}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2818--2826, 2016.
\newblock \doi{10.1109/CVPR.2016.308}.

\bibitem[Tieleman and Hinton(2012)]{Tieleman2012rmsprop}
T.~Tieleman and G.~Hinton.
\newblock {Lecture 6.5---RmsProp: Divide the gradient by a running average of
  its recent magnitude}.
\newblock COURSERA: Neural Networks for Machine Learning, 2012.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Keysers, Uszkoreit, Lucic, and
  Dosovitskiy]{tolstikhin2021mlpmixer}
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,
  Thomas Unterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario
  Lucic, and Alexey Dosovitskiy.
\newblock Mlp-mixer: An all-mlp architecture for vision, 2021.

\bibitem[Touvron et~al.(2021{\natexlab{a}})Touvron, Bojanowski, Caron, Cord,
  El-Nouby, Grave, Joulin, Synnaeve, Verbeek, and Jégou]{touvron2021resmlp}
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin
  El-Nouby, Edouard Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and
  Hervé Jégou.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training, 2021{\natexlab{a}}.

\bibitem[Touvron et~al.(2021{\natexlab{b}})Touvron, Cord, Douze, Massa,
  Sablayrolles, and Jégou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Hervé Jégou.
\newblock Training data-efficient image transformers \& distillation through
  attention, 2021{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{Wong2020Fast}
Eric Wong, Leslie Rice, and J.~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJx040EFvH}.

\bibitem[Xiao et~al.(2020)Xiao, Pennington, and Schoenholz]{pmlr-v119-xiao20b}
Lechao Xiao, Jeffrey Pennington, and Samuel Schoenholz.
\newblock Disentangling trainability and generalization in deep neural
  networks.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 10462--10472. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/xiao20b.html}.

\bibitem[Xie et~al.(2020)Xie, Tan, Gong, Wang, Yuille, and Le]{xie2020advprop}
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan~L. Yuille, and Quoc~V.
  Le.
\newblock Adversarial examples improve image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Yang and Schoenholz(2017)]{yang2017mean}
Ge~Yang and Samuel Schoenholz.
\newblock Mean field residual networks: On the edge of chaos.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/81c650caac28cdefce4de5ddc18befa0-Paper.pdf}.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{You2020Large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Syx4wnEtvH}.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Wang, Yu, Shi, Jiang, Tay, Feng, and
  Yan]{yuan2021tokenstotoken}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis~EH
  Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet, 2021.

\bibitem[Yun et~al.(2019)Yun, Han, Chun, Oh, Yoo, and Choe]{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong~Joon Oh, Youngjoon Yoo, and
  Junsuk Choe.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{2019 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 6022--6031, 2019.
\newblock \doi{10.1109/ICCV.2019.00612}.

\bibitem[Zela et~al.(2020)Zela, Elsken, Saikia, Marrakchi, Brox, and
  Hutter]{Zela2020Understanding}
Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and
  Frank Hutter.
\newblock Understanding and robustifying differentiable architecture search.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1gDNyrKDS}.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N. Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=r1Ddp1-Rb}.

\end{thebibliography}
