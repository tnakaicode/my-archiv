ReplAce:
Wirelength and density, but the way cell density is considered is with both global and local optimizers. The global density
penalty is applied equally throughout the layout, and the local density penalty is a customized penalty at a per-cell granularity. 


They can produce the best result in the best runtime. But, the analytical approach has three problems. First, only an approximation (e.g., by log-sum-exp or quadratic function) of the HPWL is minimized. Second, the distribution of objects is also approximated and this usually results in a large amount of overlaps. They have to rely on a legalization step to resolve the overlaps. For mixed-size designs, such a legalization process is very difficult and is likely to significantly increase the wirelength, because a small movement of one big macro can potentially affect the locations of thousands of small standard cells. Third, traditional analytical placers cannot optimize macro-orientations and handle geometry constraints. Most recently, Hsu and Chang [2010] improved the traditional analytical placement algorithm by introducing a rotation force to optimize the macro-orientations. But their algorithm still cannot handle geometry constraints.

Other researchers apply a two-stage approach as shown in Figure 2 to handle the mixed-size placement. An initial wirelength-driven placement is first generated. Then a macroplacement or legalization algorithm is used to place only the macros, without considering the standard cells. After this, the macros are fixed and the standard cells are replaced in the remaining whitespace from scratch. As the macroplacement is a crucial stage in this flow, people propose different techniques to improve the quality of result (QoR). Based on the multi-packing tree (MPT) representation, Chen et al. [2007] used a packing-based algorithm to place the macros around the four corners of the chip region. In Chen et al. [2008], a transitive closure graph (TCG) based technique was applied to enhance the quality of macroplacement. In Cong and Xie [2006], the authors applied a combination of constraint graph and linear programming approaches to obtain a legalized placement on the macros. One main problem with the preceding three approaches is that the initial placement is produced with a large amount of overlaps. Thus, the initial solution may not provide good indications on the locations of objects. . However, the following macroplacement stage still determines the macro locations by minimizing the displacement from the low-quality initial placement.
Alternatively, after the initial placement is generated by a standard-cell placer, Adya and Markov [2005] used an annealing-based fixed-outline floorplanner to remove the overlap between the macros and clustered standard cells at the macroplacement stage. But, they still have to rely on the illegal placement to determine the initial locations of macros and clusters. For all of the previous two-stage approaches, after fixing the macros, the initial positions of standard cells have to be discarded to reduce the overlaps.



In this section, we define the reward $r$, state $s$, actions $a$, policy architecture $\pi_{\theta}(a|s)$ parameterized by $\theta$, and finally the optimization method we use to train those parameters.

\subsection{State Representation}
Figure \ref{fig:architecture} shows the overall RL architecture. Our state contains information about the candidate node (macro) to be placed: the embedding of that node, embedding of the netlist, and some metadata about the netlist including the total number of wires, macros, and standard cell groups. In the following we discuss how we generate node and graph embeddings for this problem.


\subsection{Policy architecture} In this section, we propose a new graph neural network method that embeds information about the netlist. The role of graph neural networks is to distill information about the type and adjacency structure of a node within a large graph into low-dimensional vector representations which can later be used in various downstream tasks such as node classification \citep{}, device placement \citep{}, and link prediction \citep{}. In a recent work, graph neural networks have been used to predict DRC from a netlist once the placement is done and before doing detailed routing \citep{}.

In the following, we propose a new graph neural method that suits the placement problem. The input to this graph neural network is the netlist's adjacency graph and node types (macros vs. standard cell groups). We explain the steps in Algorithm \ref{ref:algorithm}. The input to the algorithm is information about the node features including the type (macro vs. standard cell cluster), node width, height. We also get node neighborhood information as input. We first create a vectorized representation of the nodes by concatenating the node features. We then repeatedly perform the following updates: first each edge updates its representation by applying a fully connected network on an aggregated representation of node intermediate embeddings. We then for each node, take the mean of the embeddings of its adjacent edges and update the node representation.
 
\begin{algorithm}[H]
\label{ref:algorithm}
\SetAlgoLined
\KwResult{}
 initialization: netlist's adjacency graph and node types (macros vs. standard cell groups)
 Node are represented with $v_i's$ for $1<=i<=N$\;
 \While{Repeat until convergence}{
  $e_{ij} = fc^1(concat[fc^0(v_i) | fc^0(vj) | w^e_{ij}])$ \;
  $v_i = mean_{j\in N(v_i)}(a_{ij})$ \;
  }
 }
 \caption{Edge based graph neural network embedding.}
\end{algorithm}

\subsection{Action representation} As described earlier, for policy optimization purposes, we convert the canvass into a $m\times n$ grid. Thus, for any given state, the action space (or the output of the policy) is the probability distribution of placement (of the current candidate macro) over the $m\times n$ grid. The action is the argmax of this probability distribution. 

\textbf{Density Constraints}: A feasible node (macro or standardd cell group) placement should meet the following criteria: the density of the placed items on each grid cell should not be more than a target density threshold ($max_{density}$. To meet this criteria, during each RL step we calculate the current density mask, a binary $m\times n$ matrix that represents grid cells onto which we can place the center of the current node without violating the density threshold criteria. Before choosing an action from the policy output, we first take the dot product of the mask and the policy output and then take the argmax from the feasible locations. This approach inherently disables the creation of macros to overlap, or dense standard cell areas.


\subsection{Policy Update: Training Parameters $\theta$}
To train the parameters of the policy, we use Proximal Policy Optimization (PPO) \citep{}. 


